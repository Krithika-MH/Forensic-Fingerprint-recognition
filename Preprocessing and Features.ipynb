{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9fe5b5-8d97-4764-9c88-522105a0017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#om gan ganapathaye namah om namah shivaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f51e1d6-bae7-455c-b67f-810b1942ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 110540 rows to D:\\5th sem\\mini_project\\dataset\\SOCOFing\\socofing_index.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re \n",
    "\n",
    "# ====== HELPER: PARSE FILENAME (FINAL CORRECTION) ======\n",
    "def parse_filename(fname_stem, real_or_altered):\n",
    "    \"\"\"\n",
    "    Parse SOCOFing filename stem (without extension).\n",
    "\n",
    "    Handles:\n",
    "    1. Multiple underscores (e.g., '100__M' -> '100_M').\n",
    "    2. Finger name split into two parts (e.g., 'index_finger') for both real and altered files.\n",
    "    \"\"\"\n",
    "    # 1. Clean the stem by replacing multiple underscores with a single underscore\n",
    "    cleaned_stem = re.sub(r\"__+\", \"_\", fname_stem)\n",
    "    parts = cleaned_stem.split(\"_\")\n",
    "\n",
    "    if real_or_altered == \"real\":\n",
    "        # Expected correct parts: 4 (e.g., 100_M_Left_little)\n",
    "        # Observed inconsistent parts: 5 (e.g., 100_M_Left_index_finger)\n",
    "        \n",
    "        if len(parts) == 4:\n",
    "            subject_id, gender, hand, finger = parts\n",
    "        elif len(parts) == 5:\n",
    "            # Reconstruct the 'finger' name by joining the last two parts\n",
    "            subject_id, gender, hand = parts[0], parts[1], parts[2]\n",
    "            finger = \"_\".join(parts[3:]) # Joins 'index' and 'finger' back to 'index_finger'\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected real filename format after cleanup: {fname_stem} -> {cleaned_stem}. Parts: {parts}\")\n",
    "            \n",
    "        alteration_type = \"none\"\n",
    "        \n",
    "    else:\n",
    "        # Expected correct parts: 5 (e.g., 100_M_Left_little_CR)\n",
    "        # Observed inconsistent parts: 6 (e.g., 100_M_Left_index_finger_CR)\n",
    "        \n",
    "        if len(parts) == 5:\n",
    "            subject_id, gender, hand, finger, alteration_type = parts\n",
    "        elif len(parts) == 6:\n",
    "            # Reconstruct the 'finger' name by joining the 4th and 5th parts (index 3 and 4)\n",
    "            subject_id, gender, hand = parts[0], parts[1], parts[2]\n",
    "            finger = \"_\".join(parts[3:5]) # Joins 'index' and 'finger'\n",
    "            alteration_type = parts[5] # The last part is the alteration type\n",
    "        else:\n",
    "             raise ValueError(f\"Unexpected altered filename format after cleanup: {fname_stem} -> {cleaned_stem}. Parts: {parts}\")\n",
    "\n",
    "    return {\n",
    "        \"subject_id\": subject_id,\n",
    "        \"gender\": gender,\n",
    "        \"hand\": hand,\n",
    "        \"finger\": finger,\n",
    "        \"alteration_type\": alteration_type,\n",
    "    }\n",
    "\n",
    "# ... (The rest of the code for build_index() and __main__ should be placed below) ...\n",
    "\n",
    "# ====== CONFIGURE THIS ======\n",
    "# Change this to the root folder where your SOCOFing data is stored\n",
    "# Example structure:\n",
    "# Â  DATA_ROOT/\n",
    "# Â  Â  Â  Real/\n",
    "# Â  Â  Â  Altered/\n",
    "# Â  Â  Â  Â  Â  Altered-Easy/\n",
    "# Â  Â  Â  Â  Â  Altered-Medium/\n",
    "# Â  Â  Â  Â  Â  Altered-Hard/\n",
    "DATA_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\SOCOFing\" \n",
    "\n",
    "# Output CSV path\n",
    "OUTPUT_CSV = r\"D:\\5th sem\\mini_project\\dataset\\SOCOFing\\socofing_index.csv\"\n",
    "\n",
    "# Subfolders (relative to DATA_ROOT) and their corresponding alter_level labels\n",
    "SUBFOLDERS = [\n",
    "    (\"Real\", \"none\"),\n",
    "    (\"Altered/Altered-Easy\", \"easy\"),\n",
    "    (\"Altered/Altered-Medium\", \"medium\"),\n",
    "    (\"Altered/Altered-Hard\", \"hard\"),\n",
    "]\n",
    "\n",
    "\n",
    "# ====== MAIN SCRIPT ======\n",
    "def build_index():\n",
    "    rows = []\n",
    "\n",
    "    for rel_subfolder, alter_level in SUBFOLDERS:\n",
    "        folder_path = Path(DATA_ROOT) / rel_subfolder\n",
    "\n",
    "        if not folder_path.exists():\n",
    "            print(f\"Warning: folder not found -> {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        # Decide real_or_altered flag\n",
    "        real_or_altered = \"real\" if alter_level == \"none\" else \"altered\"\n",
    "\n",
    "        # Iterate over all common image extensions\n",
    "        for ext in (\"*.bmp\", \"*.BMP\", \"*.png\", \"*.jpg\", \"*.jpeg\"):\n",
    "            for img_path in folder_path.glob(ext):\n",
    "                fname_stem = img_path.stem\n",
    "\n",
    "                info = parse_filename(fname_stem, real_or_altered)\n",
    "\n",
    "                row = {\n",
    "                    \"image_path\": str(img_path.resolve()),\n",
    "                    \"subject_id\": info[\"subject_id\"],\n",
    "                    \"finger\": info[\"finger\"],\n",
    "                    \"real_or_altered\": real_or_altered,\n",
    "                    \"alter_level\": alter_level,  # none / easy / medium / hard\n",
    "                    # Extra useful fields:\n",
    "                    \"gender\": info[\"gender\"],\n",
    "                    \"hand\": info[\"hand\"],\n",
    "                    \"alteration_type\": info[\"alteration_type\"],\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "    # Write CSV\n",
    "    fieldnames = [\n",
    "        \"image_path\",\n",
    "        \"subject_id\",\n",
    "        \"finger\",\n",
    "        \"real_or_altered\",\n",
    "        \"alter_level\",\n",
    "        \"gender\",\n",
    "        \"hand\",\n",
    "        \"alteration_type\",\n",
    "    ]\n",
    "\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Saved {len(rows)} rows to {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056608c-db9f-4d18-9912-24d79b8eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\SOCOFing\"\n",
    "OUT_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\NPYSF\"\n",
    "IMG_SIZE = (512, 512)\n",
    "\n",
    "# --- FOLDER DEFINITIONS ---\n",
    "folders = [\n",
    "    (\"Real\", \"Real_npys\"),\n",
    "    (r\"Altered\\Altered-Easy\", \"Altered_Easy_npys\"),\n",
    "    (r\"Altered\\Altered-Medium\", \"Altered_Medium_npys\"),\n",
    "    (r\"Altered\\Altered-Hard\", \"Altered_Hard_npys\")\n",
    "]\n",
    "\n",
    "# Create output base directory once\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# SIFT object is created once, using default settings (no nfeatures limit)\n",
    "sift = cv2.SIFT_create() \n",
    "\n",
    "# --- CORE FEATURE EXTRACTION LOGIC (UNCHANGED) ---\n",
    "def extract_aggregate_vector(img_path):\n",
    "    \"\"\"\n",
    "    Extracts, enhances, detects SIFT features, aggregates them,\n",
    "    and returns a centered, normalized vector.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path, 0) # Read as grayscale\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    # Image Preprocessing: Resize and Histogram Equalization\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    \n",
    "    # SIFT Detection and Computation\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    if des is None or len(des) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Aggregation (Mean Vector)\n",
    "    v = np.mean(des.astype(np.float32), axis=0)\n",
    "    \n",
    "    # Centering the Vector (v = v - mean(v)) - LOGIC PRESERVED\n",
    "    v = v - np.mean(v)\n",
    "    \n",
    "    # Normalization (L2 Norm)\n",
    "    n = np.linalg.norm(v)\n",
    "    \n",
    "    if n < 1e-12:\n",
    "        return None\n",
    "        \n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "# --- MAIN PROCESSING FUNCTION ---\n",
    "def process_dataset_folder(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Walks through input_folder, extracts features from all images,\n",
    "    and saves them to the corresponding path in output_folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\nScanning: {input_folder}\")\n",
    "    \n",
    "    # 1. Collect all image file paths first\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith((\".bmp\", \".png\", \".jpg\", \".jpeg\")):\n",
    "                # Store (full path, relative path from input_folder)\n",
    "                full_path = os.path.join(root, fn)\n",
    "                rel_path = os.path.relpath(full_path, input_folder)\n",
    "                all_files.append((full_path, rel_path))\n",
    "\n",
    "    print(f\"Total images found: {len(all_files)}\")\n",
    "    \n",
    "    # 2. Process files with a clean progress bar\n",
    "    for img_path, rel_path in tqdm(all_files, desc=f\"Processing {os.path.basename(input_folder)}\", colour=\"green\"):\n",
    "        \n",
    "        # Determine output path, maintaining subfolder structure\n",
    "        base_name, _ = os.path.splitext(rel_path)\n",
    "        out_path = os.path.join(output_folder, base_name + \".npy\")\n",
    "        \n",
    "        # Create necessary output subdirectories\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        \n",
    "        # Skip if the NPY file already exists\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "            \n",
    "        # Extract features and save\n",
    "        agg_vector = extract_aggregate_vector(img_path)\n",
    "        \n",
    "        if agg_vector is not None:\n",
    "            np.save(out_path, agg_vector)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(\"================= STARTING FEATURE EXTRACTION =================\\n\")\n",
    "\n",
    "for in_rel, out_rel in folders:\n",
    "    input_dir = os.path.join(RAW_ROOT, in_rel)\n",
    "    output_dir = os.path.join(OUT_ROOT, out_rel)\n",
    "    \n",
    "    process_dataset_folder(input_dir, output_dir)\n",
    "\n",
    "print(\"\\nðŸŽ‰ ALL DONE. NPYSF regenerated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098dcf63-1c4b-4379-b5f7-69117ce1621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= STARTING FULL DESCRIPTOR EXTRACTION =================\n",
      "\n",
      "\n",
      "Scanning: D:\\5th sem\\mini_project\\dataset\\SOCOFing\\Real\n",
      "Total images found: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full SIFT Real: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6000/6000 [12:14<00:00,  8.17it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: D:\\5th sem\\mini_project\\dataset\\SOCOFing\\Altered\\Altered-Easy\n",
      "Total images found: 17931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full SIFT Altered_Easy: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 17931/17931 [30:21<00:00,  9.84it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: D:\\5th sem\\mini_project\\dataset\\SOCOFing\\Altered\\Altered-Medium\n",
      "Total images found: 17067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full SIFT Altered_Medium: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 17067/17067 [28:07<00:00, 10.11it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: D:\\5th sem\\mini_project\\dataset\\SOCOFing\\Altered\\Altered-Hard\n",
      "Total images found: 14272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full SIFT Altered_Hard: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 14272/14272 [23:09<00:00, 10.27it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ALL DONE. Full SIFT descriptors saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\SOCOFing\"\n",
    "DES_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\NPYSF_full_des\"  # NEW folder for full descriptors\n",
    "IMG_SIZE = (512, 512)\n",
    "\n",
    "# --- FOLDER DEFINITIONS (same logical splits as before) ---\n",
    "folders = [\n",
    "    (\"Real\", \"Real\"),\n",
    "    (r\"Altered\\Altered-Easy\", \"Altered_Easy\"),\n",
    "    (r\"Altered\\Altered-Medium\", \"Altered_Medium\"),\n",
    "    (r\"Altered\\Altered-Hard\", \"Altered_Hard\")\n",
    "]\n",
    "\n",
    "os.makedirs(DES_ROOT, exist_ok=True)\n",
    "\n",
    "# SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Read, grayscale, resize, and enhance fingerprint image.\"\"\"\n",
    "    img = cv2.imread(img_path, 0)  # grayscale\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    return img\n",
    "\n",
    "def extract_descriptors(img_path):\n",
    "    \"\"\"Return full SIFT descriptor array [N, 128] or None.\"\"\"\n",
    "    img = preprocess_image(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    if des is None or len(des) == 0:\n",
    "        return None\n",
    "\n",
    "    return des.astype(np.float32)\n",
    "\n",
    "def process_dataset_folder(input_folder, tag):\n",
    "    \"\"\"Save full descriptors for all images in a given folder.\"\"\"\n",
    "    print(f\"\\nScanning: {input_folder}\")\n",
    "\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith((\".bmp\", \".png\", \".jpg\", \".jpeg\")):\n",
    "                full_path = os.path.join(root, fn)\n",
    "                rel_path = os.path.relpath(full_path, input_folder)\n",
    "                all_files.append((full_path, rel_path))\n",
    "\n",
    "    print(f\"Total images found: {len(all_files)}\")\n",
    "\n",
    "    for img_path, rel_path in tqdm(all_files,\n",
    "                                   desc=f\"Full SIFT {tag}\",\n",
    "                                   colour=\"blue\"):\n",
    "        base_name, _ = os.path.splitext(rel_path)\n",
    "\n",
    "        # Output path for descriptors (mirror subfolders)\n",
    "        des_out = os.path.join(DES_ROOT, tag, base_name + \".npy\")\n",
    "        os.makedirs(os.path.dirname(des_out), exist_ok=True)\n",
    "\n",
    "        # Skip if already processed\n",
    "        if os.path.exists(des_out):\n",
    "            continue\n",
    "\n",
    "        des = extract_descriptors(img_path)\n",
    "        if des is not None:\n",
    "            np.save(des_out, des)\n",
    "\n",
    "print(\"================= STARTING FULL DESCRIPTOR EXTRACTION =================\\n\")\n",
    "\n",
    "for in_rel, tag in folders:\n",
    "    input_dir = os.path.join(RAW_ROOT, in_rel)\n",
    "    process_dataset_folder(input_dir, tag)\n",
    "\n",
    "print(\"\\nALL DONE. Full SIFT descriptors saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d51809-fc62-4756-8ca9-fc825de16150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== STEP 5A: KEYPOINT STATS =====\n",
      "\n",
      "=== REAL ===\n",
      "  Files: 6000\n",
      "  num_keypoints min: 685\n",
      "  num_keypoints max: 4425\n",
      "  num_keypoints mean: 1772.72\n",
      "\n",
      "=== EASY ===\n",
      "  Files: 17931\n",
      "  num_keypoints min: 504\n",
      "  num_keypoints max: 4443\n",
      "  num_keypoints mean: 1728.86\n",
      "\n",
      "=== MEDIUM ===\n",
      "  Files: 17067\n",
      "  num_keypoints min: 434\n",
      "  num_keypoints max: 4437\n",
      "  num_keypoints mean: 1689.70\n",
      "\n",
      "=== HARD ===\n",
      "  Files: 14272\n",
      "  num_keypoints min: 386\n",
      "  num_keypoints max: 4418\n",
      "  num_keypoints mean: 1649.13\n",
      "\n",
      "===== BUILDING INDEX FOR MATCHING TESTS =====\n",
      "\n",
      "Indexed 55270 descriptor files in total.\n",
      "Number of (subject_id, finger) groups: 3000\n",
      "\n",
      "===== STEP 5B: GENUINE VS IMPOSTOR MATCHING =====\n",
      "\n",
      "===== GENUINE MATCHES (same subject, same finger) =====\n",
      "  Genuine pair: subj=572 finger=thumb_finger (easy vs real) -> good_matches=1472\n",
      "  Genuine pair: subj=303 finger=little_finger (easy vs hard) -> good_matches=680\n",
      "  Genuine pair: subj=203 finger=little_finger (easy vs easy) -> good_matches=899\n",
      "  Genuine pair: subj=536 finger=ring_finger (medium vs real) -> good_matches=1446\n",
      "  Genuine pair: subj=122 finger=middle_finger (easy vs easy) -> good_matches=22\n",
      "\n",
      "===== IMPOSTOR MATCHES (different subjects) =====\n",
      "  Impostor pair: subj1=315 subj2=567 (hard vs real) -> good_matches=10\n",
      "  Impostor pair: subj1=77 subj2=284 (hard vs hard) -> good_matches=19\n",
      "  Impostor pair: subj1=488 subj2=303 (medium vs hard) -> good_matches=18\n",
      "  Impostor pair: subj1=357 subj2=106 (easy vs hard) -> good_matches=18\n",
      "  Impostor pair: subj1=490 subj2=414 (easy vs easy) -> good_matches=16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import re # <-- Added import for regular expressions\n",
    "\n",
    "# ========= CONFIGURE THESE =========\n",
    "# Folder where you saved full descriptors (from NPYSF_full_des script)\n",
    "DES_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\NPYSF_full_des\"\n",
    "\n",
    "# Subfolders inside DES_ROOT as used earlier\n",
    "TAG_FOLDERS = {\n",
    "    \"real\": \"Real\",\n",
    "    \"easy\": \"Altered_Easy\",\n",
    "    \"medium\": \"Altered_Medium\",\n",
    "    \"hard\": \"Altered_Hard\",\n",
    "}\n",
    "\n",
    "# For matching tests\n",
    "NUM_GENUINE_TESTS = 5    # how many genuine pairs to test\n",
    "NUM_IMPOSTOR_TESTS = 5 # how many impostor pairs to test\n",
    "\n",
    "# ========= HELPER: PARSE FILENAME (CORRECTED) =========\n",
    "def parse_filename_from_stem(stem):\n",
    "    \"\"\"\n",
    "    Parse filename stem to extract subject_id, finger, gender, and hand.\n",
    "    Handles: \n",
    "      1. Multiple underscores (e.g., '100__M' -> '100_M').\n",
    "      2. Finger name split into two parts (e.g., 'index_finger' leading to 5 or 6 parts total).\n",
    "    \"\"\"\n",
    "    # 1. Clean the stem by replacing multiple underscores with a single underscore\n",
    "    cleaned_stem = re.sub(r\"__+\", \"_\", stem)\n",
    "    parts = cleaned_stem.split(\"_\")\n",
    "    \n",
    "    # Analyze the length to determine the structure:\n",
    "    # 4 parts: [subj, gender, hand, finger] (e.g., 100_M_Left_little)\n",
    "    # 5 parts: [subj, gender, hand, finger_part1, finger_part2] OR [subj, gender, hand, finger, alter_type]\n",
    "    # 6 parts: [subj, gender, hand, finger_part1, finger_part2, alter_type]\n",
    "\n",
    "    if len(parts) < 4:\n",
    "        raise ValueError(f\"Unexpected filename format: {stem}\")\n",
    "    \n",
    "    subject_id = parts[0]\n",
    "    gender = parts[1]\n",
    "    hand = parts[2]\n",
    "    \n",
    "    # Check if the finger name is split (parts[3] usually 'index' or 'middle', parts[4] usually 'finger')\n",
    "    # If the file is altered (5 or 6 parts) or real (5 parts), the finger name is likely split\n",
    "    if len(parts) >= 5 and parts[4].lower() in ['finger', 'thumb', 'index', 'middle', 'ring', 'little']:\n",
    "        # If the 5th part is a finger name, we assume parts[3] and parts[4] must be joined.\n",
    "        finger = \"_\".join(parts[3:5]) # e.g., 'index' + 'finger' -> 'index_finger'\n",
    "    else:\n",
    "        # Otherwise, the finger name is the single part[3] (e.g., 'little', or if only 4 parts)\n",
    "        finger = parts[3]\n",
    "\n",
    "    return subject_id, finger, gender, hand\n",
    "\n",
    "# ========= STEP 5A: STATS PER CATEGORY =========\n",
    "def compute_keypoint_stats():\n",
    "# ... (function body is unchanged) ...\n",
    "    stats = {} \n",
    "\n",
    "    for tag, subdir in TAG_FOLDERS.items():\n",
    "        folder = Path(DES_ROOT) / subdir\n",
    "        if not folder.exists():\n",
    "            print(f\"[WARN] Folder not found for tag '{tag}': {folder}\")\n",
    "            continue\n",
    "\n",
    "        counts = []\n",
    "        for npy_path in folder.rglob(\"*.npy\"):\n",
    "            des = np.load(str(npy_path))\n",
    "            if des is None:\n",
    "                continue\n",
    "            # des is [N, 128]\n",
    "            num_kp = des.shape[0]\n",
    "            counts.append(num_kp)\n",
    "\n",
    "        if not counts:\n",
    "            print(f\"[INFO] No descriptors found for tag '{tag}'.\")\n",
    "            continue\n",
    "\n",
    "        counts = np.array(counts, dtype=np.int32)\n",
    "        stats[tag] = counts\n",
    "\n",
    "        print(f\"\\n=== {tag.upper()} ===\")\n",
    "        print(f\"  Files: {len(counts)}\")\n",
    "        print(f\"  num_keypoints min: {counts.min()}\")\n",
    "        print(f\"  num_keypoints max: {counts.max()}\")\n",
    "        print(f\"  num_keypoints mean: {counts.mean():.2f}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ========= BUILD INDEX FOR MATCHING TESTS =========\n",
    "def build_descriptor_index():\n",
    "# ... (function body is unchanged) ...\n",
    "    all_items = []\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for tag, subdir in TAG_FOLDERS.items():\n",
    "        folder = Path(DES_ROOT) / subdir\n",
    "        if not folder.exists():\n",
    "            continue\n",
    "\n",
    "        for npy_path in folder.rglob(\"*.npy\"):\n",
    "            stem = npy_path.stem  # filename without .npy\n",
    "            try:\n",
    "                # This will use the corrected parsing function\n",
    "                subject_id, finger, gender, hand = parse_filename_from_stem(stem) \n",
    "            except ValueError as e:\n",
    "                print(f\"[WARN] Skipping {npy_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            idx = len(all_items)\n",
    "            all_items.append({\n",
    "                \"path\": str(npy_path),\n",
    "                \"tag\": tag,\n",
    "                \"subject_id\": subject_id,\n",
    "                \"finger\": finger,\n",
    "                \"gender\": gender,\n",
    "                \"hand\": hand,\n",
    "            })\n",
    "            groups[(subject_id, finger)].append(idx)\n",
    "\n",
    "    print(f\"\\nIndexed {len(all_items)} descriptor files in total.\")\n",
    "    print(f\"Number of (subject_id, finger) groups: {len(groups)}\")\n",
    "    return all_items, groups\n",
    "\n",
    "# ========= HELPER: BF + RATIO-TEST MATCHING =========\n",
    "def count_good_matches(des1, des2, ratio_thresh=0.75):\n",
    "# ... (function body is unchanged) ...\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    # des1, des2: float32 arrays [N1, 128], [N2, 128]\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good.append(m)\n",
    "    return len(good)\n",
    "\n",
    "# ========= STEP 5B: GENUINE VS IMPOSTOR MATCHING TESTS =========\n",
    "def test_matching(all_items, groups):\n",
    "# ... (function body is unchanged) ...\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    # Build a list of group keys with at least 2 descriptors (for genuine)\n",
    "    genuine_groups = [k for k, idxs in groups.items() if len(idxs) >= 2]\n",
    "    if not genuine_groups:\n",
    "        print(\"[ERROR] No (subject, finger) group has >= 2 descriptors.\")\n",
    "        return\n",
    "\n",
    "    # Build a list of all indices by subject_id for impostors\n",
    "    subject_to_indices = defaultdict(list)\n",
    "    for i, item in enumerate(all_items):\n",
    "        subject_to_indices[item[\"subject_id\"]].append(i)\n",
    "    subjects = list(subject_to_indices.keys())\n",
    "\n",
    "    print(\"\\n===== GENUINE MATCHES (same subject, same finger) =====\")\n",
    "    for _ in range(NUM_GENUINE_TESTS):\n",
    "        # Pick a group and then two different indices from that group\n",
    "        key = rng.choice(genuine_groups)\n",
    "        idxs = groups[key]\n",
    "        if len(idxs) < 2:\n",
    "            continue\n",
    "        i1, i2 = rng.sample(idxs, 2)\n",
    "\n",
    "        item1 = all_items[i1]\n",
    "        item2 = all_items[i2]\n",
    "\n",
    "        des1 = np.load(item1[\"path\"])\n",
    "        des2 = np.load(item2[\"path\"])\n",
    "\n",
    "        good = count_good_matches(des1, des2)\n",
    "\n",
    "        print(f\"  Genuine pair: subj={item1['subject_id']} finger={item1['finger']} \"\n",
    "              f\"({item1['tag']} vs {item2['tag']}) -> good_matches={good}\")\n",
    "\n",
    "    print(\"\\n===== IMPOSTOR MATCHES (different subjects) =====\")\n",
    "    for _ in range(NUM_IMPOSTOR_TESTS):\n",
    "        # Pick two different subjects\n",
    "        subj1, subj2 = rng.sample(subjects, 2)\n",
    "        i1 = rng.choice(subject_to_indices[subj1])\n",
    "        i2 = rng.choice(subject_to_indices[subj2])\n",
    "\n",
    "        item1 = all_items[i1]\n",
    "        item2 = all_items[i2]\n",
    "\n",
    "        des1 = np.load(item1[\"path\"])\n",
    "        des2 = np.load(item2[\"path\"])\n",
    "\n",
    "        good = count_good_matches(des1, des2)\n",
    "\n",
    "        print(f\"  Impostor pair: subj1={item1['subject_id']} subj2={item2['subject_id']} \"\n",
    "              f\"({item1['tag']} vs {item2['tag']}) -> good_matches={good}\")\n",
    "\n",
    "# ========= MAIN =========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"===== STEP 5A: KEYPOINT STATS =====\")\n",
    "    stats = compute_keypoint_stats()\n",
    "\n",
    "    print(\"\\n===== BUILDING INDEX FOR MATCHING TESTS =====\")\n",
    "    all_items, groups = build_descriptor_index()\n",
    "\n",
    "    print(\"\\n===== STEP 5B: GENUINE VS IMPOSTOR MATCHING =====\")\n",
    "    test_matching(all_items, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19cff20c-82ab-446d-874a-cd79d617f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extended CSV to: D:\\5th sem\\mini_project\\dataset\\socofing_index_features.csv\n",
      "Rows: 110540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "CSV_PATH = r\"D:\\5th sem\\mini_project\\dataset\\socofing_index.csv\"\n",
    "\n",
    "# Roots where you saved features\n",
    "AGG_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\NPYSF\"\n",
    "DES_ROOT = r\"D:\\5th sem\\mini_project\\dataset\\NPYSF_full_des\"\n",
    "\n",
    "# Output CSV with feature paths\n",
    "OUT_CSV = r\"D:\\5th sem\\mini_project\\dataset\\socofing_index_features.csv\"\n",
    "\n",
    "# Mapping (real_or_altered, alter_level) -> subfolders\n",
    "AGG_SUBDIRS = {\n",
    "    (\"real\", \"none\"):   \"Real_npys\",\n",
    "    (\"altered\", \"easy\"):   \"Altered_Easy_npys\",\n",
    "    (\"altered\", \"medium\"): \"Altered_Medium_npys\",\n",
    "    (\"altered\", \"hard\"):   \"Altered_Hard_npys\",\n",
    "}\n",
    "\n",
    "DES_SUBDIRS = {\n",
    "    (\"real\", \"none\"):   \"Real\",\n",
    "    (\"altered\", \"easy\"):   \"Altered_Easy\",\n",
    "    (\"altered\", \"medium\"): \"Altered_Medium\",\n",
    "    (\"altered\", \"hard\"):   \"Altered_Hard\",\n",
    "}\n",
    "\n",
    "# ========= MAIN =========\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    agg_paths = []\n",
    "    des_paths = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = row[\"image_path\"]\n",
    "        real_or_altered = str(row[\"real_or_altered\"]).lower()\n",
    "        alter_level = str(row[\"alter_level\"]).lower()\n",
    "\n",
    "        key = (real_or_altered, alter_level)\n",
    "        if key not in AGG_SUBDIRS or key not in DES_SUBDIRS:\n",
    "            raise ValueError(f\"Unexpected RA/alter_level combo at row {idx}: {key}\")\n",
    "\n",
    "        # filename stem from image_path\n",
    "        img_stem = Path(image_path).stem  # e.g., \"001_M_Left_little_finger\"\n",
    "\n",
    "        # Build aggregated vector path\n",
    "        agg_sub = AGG_SUBDIRS[key]\n",
    "        agg_path = Path(AGG_ROOT) / agg_sub / (img_stem + \".npy\")\n",
    "\n",
    "        # Build full descriptor path\n",
    "        des_sub = DES_SUBDIRS[key]\n",
    "        des_path = Path(DES_ROOT) / des_sub / (img_stem + \".npy\")\n",
    "\n",
    "        agg_paths.append(str(agg_path))\n",
    "        des_paths.append(str(des_path))\n",
    "\n",
    "    df[\"agg_path\"] = agg_paths\n",
    "    df[\"des_path\"] = des_paths\n",
    "\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"Saved extended CSV to: {OUT_CSV}\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c7ef8aa-d5e1-4b62-98b1-16402414aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 110540\n",
      "Missing agg_path files: 0\n",
      "Missing des_path files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = r\"D:\\5th sem\\mini_project\\dataset\\socofing_index_features.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "missing_agg = []\n",
    "missing_des = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    agg_path = row[\"agg_path\"]\n",
    "    des_path = row[\"des_path\"]\n",
    "\n",
    "    if not os.path.exists(agg_path):\n",
    "        missing_agg.append(agg_path)\n",
    "    if not os.path.exists(des_path):\n",
    "        missing_des.append(des_path)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing agg_path files: {len(missing_agg)}\")\n",
    "print(f\"Missing des_path files: {len(missing_des)}\")\n",
    "\n",
    "if missing_agg:\n",
    "    print(\"\\nExamples of missing agg_path:\")\n",
    "    print(\"\\n\".join(missing_agg[:5]))\n",
    "\n",
    "if missing_des:\n",
    "    print(\"\\nExamples of missing des_path:\")\n",
    "    print(\"\\n\".join(missing_des[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01948c6-8a50-4dab-9bb0-9e8cded92c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf-env)",
   "language": "python",
   "name": "hf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
