{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc436db6-4a46-46c0-acd3-e8e0b5fc99ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 110540 rows.\n",
      "Total subjects: 600\n",
      "Train: 420, Val: 90, Test: 90\n",
      "\n",
      "=== Building pairs for train ===\n",
      "Positive pairs: 1404452\n",
      "Negative pairs: 1685342 (target ~1404452)\n",
      "Saved 3089794 pairs to D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_train.csv\n",
      "\n",
      "=== Building pairs for val ===\n",
      "Positive pairs: 291419\n",
      "Negative pairs: 349702 (target ~291419)\n",
      "Saved 641121 pairs to D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv\n",
      "\n",
      "=== Building pairs for test ===\n",
      "Positive pairs: 302579\n",
      "Negative pairs: 363094 (target ~302579)\n",
      "Saved 665673 pairs to D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# ========= CONFIG =========\n",
    "INDEX_CSV = r\"D:\\5th sem\\mini_project\\dataset\\socofing_index_features.csv\"\n",
    "OUT_DIR = r\"D:\\5th sem\\mini_project\\dataset\\pairs\"\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "\n",
    "# ========= STEP 1: SUBJECT-LEVEL SPLIT =========\n",
    "def split_subjects(df):\n",
    "    subjects = sorted(df[\"subject_id\"].unique().tolist())\n",
    "    rng.shuffle(subjects)\n",
    "\n",
    "    n = len(subjects)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    subj_train = set(subjects[:n_train])\n",
    "    subj_val = set(subjects[n_train:n_train + n_val])\n",
    "    subj_test = set(subjects[n_train + n_val:])\n",
    "\n",
    "    print(f\"Total subjects: {n}\")\n",
    "    print(f\"Train: {len(subj_train)}, Val: {len(subj_val)}, Test: {len(subj_test)}\")\n",
    "\n",
    "    df_train = df[df[\"subject_id\"].isin(subj_train)].copy()\n",
    "    df_val = df[df[\"subject_id\"].isin(subj_val)].copy()\n",
    "    df_test = df[df[\"subject_id\"].isin(subj_test)].copy()\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# ========= STEP 2: POSITIVE PAIRS =========\n",
    "def build_positive_pairs(df_split):\n",
    "    \"\"\"\n",
    "    Build all positive pairs within each (subject_id, finger) group.\n",
    "    Returns a list of dicts with agg_path_1, agg_path_2, label=1, etc.\n",
    "    \"\"\"\n",
    "    pos_pairs = []\n",
    "\n",
    "    # group by subject_id + finger\n",
    "    grouped = df_split.groupby([\"subject_id\", \"finger\"])\n",
    "    for (subj, finger), group in grouped:\n",
    "        # if only one sample for this finger, skip\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        # create all combinations of rows within this group\n",
    "        indices = list(group.index)\n",
    "        for i, j in combinations(indices, 2):\n",
    "            r1 = df_split.loc[i]\n",
    "            r2 = df_split.loc[j]\n",
    "\n",
    "            pos_pairs.append({\n",
    "                \"agg_path_1\": r1[\"agg_path\"],\n",
    "                \"agg_path_2\": r2[\"agg_path\"],\n",
    "                \"label\": 1,\n",
    "                \"subject_id_1\": r1[\"subject_id\"],\n",
    "                \"subject_id_2\": r2[\"subject_id\"],\n",
    "                \"finger_1\": r1[\"finger\"],\n",
    "                \"finger_2\": r2[\"finger\"],\n",
    "                \"alter_level_1\": r1[\"alter_level\"],\n",
    "                \"alter_level_2\": r2[\"alter_level\"],\n",
    "            })\n",
    "\n",
    "    print(f\"Positive pairs: {len(pos_pairs)}\")\n",
    "    return pos_pairs\n",
    "\n",
    "# ========= STEP 3: NEGATIVE PAIRS =========\n",
    "def build_negative_pairs(df_split, num_pos_pairs, max_neg_factor=1.2):\n",
    "    \"\"\"\n",
    "    Build random negative (impostor) pairs.\n",
    "    Aim for roughly num_pos_pairs negatives (limited by max_neg_factor * num_pos_pairs).\n",
    "    \"\"\"\n",
    "    neg_pairs = []\n",
    "\n",
    "    # index by subject_id\n",
    "    subj_to_indices = defaultdict(list)\n",
    "    for idx, row in df_split.iterrows():\n",
    "        subj_to_indices[row[\"subject_id\"]].append(idx)\n",
    "\n",
    "    subjects = list(subj_to_indices.keys())\n",
    "    total_neg_target = int(min(len(df_split)**2, num_pos_pairs * max_neg_factor))\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = total_neg_target * 10  # safety\n",
    "\n",
    "    while len(neg_pairs) < total_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        subj1, subj2 = rng.sample(subjects, 2)\n",
    "        idx1 = rng.choice(subj_to_indices[subj1])\n",
    "        idx2 = rng.choice(subj_to_indices[subj2])\n",
    "\n",
    "        r1 = df_split.loc[idx1]\n",
    "        r2 = df_split.loc[idx2]\n",
    "\n",
    "        neg_pairs.append({\n",
    "            \"agg_path_1\": r1[\"agg_path\"],\n",
    "            \"agg_path_2\": r2[\"agg_path\"],\n",
    "            \"label\": 0,\n",
    "            \"subject_id_1\": r1[\"subject_id\"],\n",
    "            \"subject_id_2\": r2[\"subject_id\"],\n",
    "            \"finger_1\": r1[\"finger\"],\n",
    "            \"finger_2\": r2[\"finger\"],\n",
    "            \"alter_level_1\": r1[\"alter_level\"],\n",
    "            \"alter_level_2\": r2[\"alter_level\"],\n",
    "        })\n",
    "\n",
    "    print(f\"Negative pairs: {len(neg_pairs)} (target ~{num_pos_pairs})\")\n",
    "    return neg_pairs\n",
    "\n",
    "# ========= MAIN GENERATION =========\n",
    "def generate_pairs_for_split(df_split, split_name):\n",
    "    print(f\"\\n=== Building pairs for {split_name} ===\")\n",
    "    pos_pairs = build_positive_pairs(df_split)\n",
    "    neg_pairs = build_negative_pairs(df_split, num_pos_pairs=len(pos_pairs))\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    rng.shuffle(all_pairs)\n",
    "\n",
    "    df_pairs = pd.DataFrame(all_pairs)\n",
    "    out_path = os.path.join(OUT_DIR, f\"pairs_{split_name}.csv\")\n",
    "    df_pairs.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {len(df_pairs)} pairs to {out_path}\")\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(INDEX_CSV)\n",
    "    print(f\"Loaded index with {len(df)} rows.\")\n",
    "\n",
    "    df_train, df_val, df_test = split_subjects(df)\n",
    "\n",
    "    generate_pairs_for_split(df_train, \"train\")\n",
    "    generate_pairs_for_split(df_val, \"val\")\n",
    "    generate_pairs_for_split(df_test, \"test\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd6321-2478-4a90-a454-5ec6e78ce6fc",
   "metadata": {},
   "source": [
    "Columns: agg_path_1, agg_path_2, label, subject_id_1, subject_id_2, finger_1, finger_2, alter_level_1, alter_level_2.\n",
    "\n",
    "Each row is one pair; label = 1 for positive (same subject+finger), 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67611b47-2f61-44d5-9987-aefe95e33993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: rows=3089794, positives=1404452, negatives=1685342\n",
      "                                          agg_path_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "\n",
      "                                          agg_path_2  label  subject_id_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      1           303   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      1           347   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Real_npy...      0           132   \n",
      "\n",
      "   subject_id_2       finger_1       finger_2 alter_level_1 alter_level_2  \n",
      "0           303   index_finger   index_finger        medium          hard  \n",
      "1           347  middle_finger  middle_finger        medium          hard  \n",
      "2           258   thumb_finger  little_finger          easy          none   \n",
      "\n",
      "val: rows=641121, positives=291419, negatives=349702\n",
      "                                          agg_path_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "\n",
      "                                          agg_path_2  label  subject_id_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      1             2   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      0           527   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      0           175   \n",
      "\n",
      "   subject_id_2       finger_1       finger_2 alter_level_1 alter_level_2  \n",
      "0             2  middle_finger  middle_finger        medium        medium  \n",
      "1           284   thumb_finger  middle_finger          easy          easy  \n",
      "2           312   thumb_finger  middle_finger        medium        medium   \n",
      "\n",
      "test: rows=665673, positives=302579, negatives=363094\n",
      "                                          agg_path_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...   \n",
      "\n",
      "                                          agg_path_2  label  subject_id_1  \\\n",
      "0  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Real_npy...      0           364   \n",
      "1  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      1           204   \n",
      "2  D:\\5th sem\\mini_project\\dataset\\NPYSF\\Altered_...      0           595   \n",
      "\n",
      "   subject_id_2       finger_1      finger_2 alter_level_1 alter_level_2  \n",
      "0           176  little_finger  thumb_finger          hard          none  \n",
      "1           204    ring_finger   ring_finger        medium        medium  \n",
      "2           460   thumb_finger   ring_finger          easy          hard   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = r\"D:\\5th sem\\mini_project\\dataset\\pairs\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    csv_path = os.path.join(OUT_DIR, f\"pairs_{split}.csv\")\n",
    "    assert os.path.exists(csv_path), f\"Missing: {csv_path}\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"{split}: rows={len(df)}, positives={(df['label']==1).sum()}, negatives={(df['label']==0).sum()}\")\n",
    "    print(df.head(3), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96bfcac-a490-41c1-a05a-e365f51b1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 3089794, Val pairs: 641121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FingerprintPairsDataset(Dataset):\n",
    "    def __init__(self, pairs_csv):\n",
    "        self.df = pd.read_csv(pairs_csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load aggregated SIFT vectors\n",
    "        v1 = np.load(row[\"agg_path_1\"]).astype(np.float32)\n",
    "        v2 = np.load(row[\"agg_path_2\"]).astype(np.float32)\n",
    "\n",
    "        # Ensure shape [128] -> [1, 128] or [128] as needed\n",
    "        x1 = torch.from_numpy(v1)  # [128]\n",
    "        x2 = torch.from_numpy(v2)  # [128]\n",
    "\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)  # 0 or 1\n",
    "\n",
    "        return x1, x2, label\n",
    "\n",
    "# Example: create loaders\n",
    "if __name__ == \"__main__\":\n",
    "    train_csv = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_train.csv\"\n",
    "    val_csv = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv\"\n",
    "\n",
    "    train_dataset = FingerprintPairsDataset(train_csv)\n",
    "    val_dataset = FingerprintPairsDataset(val_csv)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "    print(f\"Train pairs: {len(train_dataset)}, Val pairs: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40aefd2-55f8-429f-9cb7-f0a785120103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ===== Siamese branch: MLP on 128-D vector =====\n",
    "class SiameseBranch(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 128]\n",
    "        return self.net(x)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.branch = SiameseBranch(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.branch(x1)\n",
    "        z2 = self.branch(x2)\n",
    "        return z1, z2\n",
    "\n",
    "# ===== Contrastive loss =====\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, labels):\n",
    "        # z1, z2: [batch, emb_dim]\n",
    "        # labels: [batch] with 1 for positive, 0 for negative\n",
    "        distances = F.pairwise_distance(z1, z2)  # [batch]\n",
    "        # standard contrastive loss: positives -> small dist, negatives -> > margin\n",
    "        loss_pos = labels * torch.pow(distances, 2)\n",
    "        loss_neg = (1 - labels) * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2)\n",
    "        loss = torch.mean(loss_pos + loss_neg)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1983a3-b2bb-404e-8d09-f0cf698b4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train pairs: 3089794, Val pairs: 641121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TRAIN_CSV = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_train.csv\"\n",
    "VAL_CSV   = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv\"\n",
    "\n",
    "MODEL_DIR          = r\"D:\\5th sem\\mini_project\\dataset\\models\"\n",
    "FINAL_MODEL_PATH   = os.path.join(MODEL_DIR, \"snn_sift_agg_last.pt\")\n",
    "BEST_MODEL_PATH    = os.path.join(MODEL_DIR, \"snn_sift_agg_best.pt\")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR         = 1e-3\n",
    "EPOCHS     = 30\n",
    "MARGIN     = 1.0\n",
    "PATIENCE   = 5  # early stopping patience\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ========== DATASET ==========\n",
    "class FingerprintPairsDataset(Dataset):\n",
    "    def __init__(self, pairs_csv):\n",
    "        self.df = pd.read_csv(pairs_csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        v1 = np.load(row[\"agg_path_1\"]).astype(np.float32)\n",
    "        v2 = np.load(row[\"agg_path_2\"]).astype(np.float32)\n",
    "\n",
    "        x1 = torch.from_numpy(v1)   # [128]\n",
    "        x2 = torch.from_numpy(v2)   # [128]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)  # 0 or 1\n",
    "\n",
    "        return x1, x2, label\n",
    "\n",
    "\n",
    "# ========== MODEL ==========\n",
    "class SiameseBranch(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.branch = SiameseBranch(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.branch(x1)\n",
    "        z2 = self.branch(x2)\n",
    "        return z1, z2\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, labels):\n",
    "        # z1, z2: [batch, emb_dim]\n",
    "        # labels: [batch] in {0,1}\n",
    "        distances = F.pairwise_distance(z1, z2)  # [batch]\n",
    "        loss_pos = labels * torch.pow(distances, 2)\n",
    "        loss_neg = (1 - labels) * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2)\n",
    "        loss = torch.mean(loss_pos + loss_neg)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ========== PREPARE DATA ==========\n",
    "train_dataset = FingerprintPairsDataset(TRAIN_CSV)\n",
    "val_dataset   = FingerprintPairsDataset(VAL_CSV)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train pairs: {len(train_dataset)}, Val pairs: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# ========== INIT MODEL / LOSS / OPTIMIZER ==========\n",
    "model = SiameseNetwork(input_dim=128, embedding_dim=128).to(device)\n",
    "criterion = ContrastiveLoss(margin=MARGIN)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ========== TRAINING LOOP WITH EARLY STOPPING ==========\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_cnt  = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x1, x2, y in train_loader:\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        z1, z2 = model(x1, x2)\n",
    "        loss = criterion(z1, z2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x1.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in val_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y  = y.to(device)\n",
    "\n",
    "            z1, z2 = model(x1, x2)\n",
    "            loss = criterion(z1, z2, y)\n",
    "            val_loss += loss.item() * x1.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"train loss: {avg_train_loss:.4f}, val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ---- EARLY STOPPING CHECK ----\n",
    "    if avg_val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_cnt = 0\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  New best model saved with val loss {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        print(f\"  No val improvement. Patience {patience_cnt}/{PATIENCE}\")\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save last-epoch model as well (optional)\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"Training complete. Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best model saved to: {BEST_MODEL_PATH}\")\n",
    "print(f\"Last model saved to: {FINAL_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9445d772-5d84-4b4f-82c8-20be0c7b2c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading training dataset (with caching)...\n",
      "[D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_train.csv] unique feature files: 38768\n",
      "Loading validation dataset (with caching)...\n",
      "[D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv] unique feature files: 8171\n",
      "Train pairs: 3089794, Val pairs: 641121\n",
      "Train feature matrix shape: torch.Size([38768, 128])\n",
      "Val feature matrix shape:   torch.Size([8171, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL-E7470\\anaconda3\\envs\\hf-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train loss: 0.1220, val loss: 0.1249\n",
      "  New best model saved with val loss 0.1249\n",
      "Epoch 2/30 - train loss: 0.1137, val loss: 0.1239\n",
      "  New best model saved with val loss 0.1239\n",
      "Epoch 3/30 - train loss: 0.1112, val loss: 0.1244\n",
      "  No val improvement. Patience 1/5\n",
      "Epoch 4/30 - train loss: 0.1097, val loss: 0.1241\n",
      "  No val improvement. Patience 2/5\n",
      "Epoch 5/30 - train loss: 0.1085, val loss: 0.1236\n",
      "  New best model saved with val loss 0.1236\n",
      "Epoch 6/30 - train loss: 0.1076, val loss: 0.1245\n",
      "  No val improvement. Patience 1/5\n",
      "Epoch 7/30 - train loss: 0.1069, val loss: 0.1247\n",
      "  No val improvement. Patience 2/5\n",
      "Epoch 8/30 - train loss: 0.1061, val loss: 0.1238\n",
      "  No val improvement. Patience 3/5\n",
      "Epoch 9/30 - train loss: 0.1056, val loss: 0.1235\n",
      "  New best model saved with val loss 0.1235\n",
      "Epoch 10/30 - train loss: 0.1050, val loss: 0.1233\n",
      "  New best model saved with val loss 0.1233\n",
      "Epoch 11/30 - train loss: 0.1045, val loss: 0.1257\n",
      "  No val improvement. Patience 1/5\n",
      "Epoch 12/30 - train loss: 0.1042, val loss: 0.1248\n",
      "  No val improvement. Patience 2/5\n",
      "Epoch 13/30 - train loss: 0.1038, val loss: 0.1255\n",
      "  No val improvement. Patience 3/5\n",
      "Epoch 14/30 - train loss: 0.1036, val loss: 0.1257\n",
      "  No val improvement. Patience 4/5\n",
      "Epoch 15/30 - train loss: 0.1032, val loss: 0.1239\n",
      "  No val improvement. Patience 5/5\n",
      "Early stopping triggered.\n",
      "Training complete. Best val loss: 0.1233\n",
      "Best model saved to: D:\\5th sem\\mini_project\\models\\snn_sift_agg_best.pt\n",
      "Last model saved to: D:\\5th sem\\mini_project\\models\\snn_sift_agg_last.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TRAIN_CSV = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_train.csv\"\n",
    "VAL_CSV   = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv\"\n",
    "\n",
    "MODEL_DIR          = r\"D:\\5th sem\\mini_project\\models\"\n",
    "FINAL_MODEL_PATH   = os.path.join(MODEL_DIR, \"snn_sift_agg_last.pt\")\n",
    "BEST_MODEL_PATH    = os.path.join(MODEL_DIR, \"snn_sift_agg_best.pt\")\n",
    "\n",
    "BATCH_SIZE = 256          # try 256; if GPU has room, 512 can be faster\n",
    "LR         = 1e-3\n",
    "EPOCHS     = 30\n",
    "MARGIN     = 1.0\n",
    "PATIENCE   = 5            # early stopping\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ========== DATASET WITH CACHED FEATURES ==========\n",
    "class CachedFingerprintPairsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads all unique aggregated SIFT vectors into RAM once,\n",
    "    then each pair just indexes into this feature matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs_csv):\n",
    "        df = pd.read_csv(pairs_csv)\n",
    "        self.n_pairs = len(df)\n",
    "\n",
    "        # 1) Collect all unique paths used in this split\n",
    "        all_paths = pd.concat([df[\"agg_path_1\"], df[\"agg_path_2\"]]).unique()\n",
    "        print(f\"[{pairs_csv}] unique feature files: {len(all_paths)}\")\n",
    "\n",
    "        # 2) Build mapping path -> index\n",
    "        self.path_to_idx = {p: i for i, p in enumerate(all_paths)}\n",
    "        self.idx_to_path = list(all_paths)  # optional, for debugging\n",
    "\n",
    "        # 3) Load all features ONCE into memory\n",
    "        feats = []\n",
    "        for p in all_paths:\n",
    "            v = np.load(p).astype(np.float32)  # [128]\n",
    "            feats.append(v)\n",
    "        feats = np.stack(feats, axis=0)       # [N_images, 128]\n",
    "        self.features = torch.from_numpy(feats)  # float32 tensor\n",
    "\n",
    "        # 4) Convert pair paths to integer indices\n",
    "        self.idx1 = df[\"agg_path_1\"].map(self.path_to_idx).to_numpy(dtype=np.int64)\n",
    "        self.idx2 = df[\"agg_path_2\"].map(self.path_to_idx).to_numpy(dtype=np.int64)\n",
    "\n",
    "        # 5) Labels\n",
    "        self.labels = df[\"label\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i1 = self.idx1[idx]\n",
    "        i2 = self.idx2[idx]\n",
    "        y  = self.labels[idx]\n",
    "\n",
    "        x1 = self.features[i1]  # [128]\n",
    "        x2 = self.features[i2]  # [128]\n",
    "\n",
    "        # Clone to avoid potential in-place ops on shared tensor\n",
    "        return x1.clone(), x2.clone(), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ========== MODEL ==========\n",
    "class SiameseBranch(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.branch = SiameseBranch(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.branch(x1)\n",
    "        z2 = self.branch(x2)\n",
    "        return z1, z2\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, labels):\n",
    "        distances = F.pairwise_distance(z1, z2)  # [batch]\n",
    "        loss_pos = labels * torch.pow(distances, 2)\n",
    "        loss_neg = (1 - labels) * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2)\n",
    "        loss = torch.mean(loss_pos + loss_neg)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ========== PREPARE DATA ==========\n",
    "print(\"Loading training dataset (with caching)...\")\n",
    "train_dataset = CachedFingerprintPairsDataset(TRAIN_CSV)\n",
    "print(\"Loading validation dataset (with caching)...\")\n",
    "val_dataset   = CachedFingerprintPairsDataset(VAL_CSV)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,       # start with 0; you can try 2 or 4 later\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train pairs: {len(train_dataset)}, Val pairs: {len(val_dataset)}\")\n",
    "print(f\"Train feature matrix shape: {train_dataset.features.shape}\")\n",
    "print(f\"Val feature matrix shape:   {val_dataset.features.shape}\")\n",
    "\n",
    "\n",
    "# ========== INIT MODEL / LOSS / OPTIMIZER ==========\n",
    "model = SiameseNetwork(input_dim=128, embedding_dim=128).to(device)\n",
    "criterion = ContrastiveLoss(margin=MARGIN)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ========== TRAINING LOOP WITH EARLY STOPPING ==========\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_cnt  = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x1, x2, y in train_loader:\n",
    "        x1 = x1.to(device, non_blocking=True)\n",
    "        x2 = x2.to(device, non_blocking=True)\n",
    "        y  = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        z1, z2 = model(x1, x2)\n",
    "        loss = criterion(z1, z2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x1.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in val_loader:\n",
    "            x1 = x1.to(device, non_blocking=True)\n",
    "            x2 = x2.to(device, non_blocking=True)\n",
    "            y  = y.to(device, non_blocking=True)\n",
    "\n",
    "            z1, z2 = model(x1, x2)\n",
    "            loss = criterion(z1, z2, y)\n",
    "            val_loss += loss.item() * x1.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"train loss: {avg_train_loss:.4f}, val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ---- EARLY STOPPING CHECK ----\n",
    "    if avg_val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_cnt = 0\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  New best model saved with val loss {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        print(f\"  No val improvement. Patience {patience_cnt}/{PATIENCE}\")\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save last-epoch model as well (optional)\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"Training complete. Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best model saved to: {BEST_MODEL_PATH}\")\n",
    "print(f\"Last model saved to: {FINAL_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b41982d-2ced-4bb9-9d86-100ef8746c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded model from D:\\5th sem\\mini_project\\models\\snn_sift_agg_best.pt\n",
      "\n",
      "--- Collecting distances on VAL set ---\n",
      "[pairs_val.csv] unique feature files: 8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL-E7470\\anaconda3\\envs\\hf-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 641121 distances from pairs_val.csv\n",
      "Val distance range: [0.0000, 1.8863]\n",
      "Best threshold on VAL: 0.5308, accuracy=0.8350\n",
      "VAL TPR (GAR) at best_thr: 0.8021\n",
      "VAL FPR (FAR) at best_thr: 0.1375\n",
      "\n",
      "--- Collecting distances on TEST set ---\n",
      "[pairs_test.csv] unique feature files: 8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL-E7470\\anaconda3\\envs\\hf-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 665673 distances from pairs_test.csv\n",
      "\n",
      "TEST results at best_thr=0.5308:\n",
      "  Accuracy: 0.8347\n",
      "  TPR (GAR): 0.7964\n",
      "  FPR (FAR): 0.1334\n",
      "\n",
      "Saved distances and labels to D:\\5th sem\\mini_project\\models\\snn_eval_distances.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ========= CONFIG =========\n",
    "VAL_CSV  = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_val.csv\"\n",
    "TEST_CSV = r\"D:\\5th sem\\mini_project\\dataset\\pairs\\pairs_test.csv\"\n",
    "\n",
    "MODEL_PATH = r\"D:\\5th sem\\mini_project\\models\\snn_sift_agg_best.pt\"\n",
    "\n",
    "BATCH_SIZE = 512  # can be larger than training since eval has no backprop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========= DATASET WITH CACHED FEATURES =========\n",
    "class CachedFingerprintPairsDataset(Dataset):\n",
    "    def __init__(self, pairs_csv):\n",
    "        df = pd.read_csv(pairs_csv)\n",
    "        self.n_pairs = len(df)\n",
    "\n",
    "        all_paths = pd.concat([df[\"agg_path_1\"], df[\"agg_path_2\"]]).unique()\n",
    "        print(f\"[{os.path.basename(pairs_csv)}] unique feature files: {len(all_paths)}\")\n",
    "\n",
    "        self.path_to_idx = {p: i for i, p in enumerate(all_paths)}\n",
    "\n",
    "        feats = []\n",
    "        for p in all_paths:\n",
    "            v = np.load(p).astype(np.float32)\n",
    "            feats.append(v)\n",
    "        feats = np.stack(feats, axis=0)   # [N_images, 128]\n",
    "        self.features = torch.from_numpy(feats)\n",
    "\n",
    "        self.idx1 = df[\"agg_path_1\"].map(self.path_to_idx).to_numpy(dtype=np.int64)\n",
    "        self.idx2 = df[\"agg_path_2\"].map(self.path_to_idx).to_numpy(dtype=np.int64)\n",
    "        self.labels = df[\"label\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i1 = self.idx1[idx]\n",
    "        i2 = self.idx2[idx]\n",
    "        y  = self.labels[idx]\n",
    "\n",
    "        x1 = self.features[i1]\n",
    "        x2 = self.features[i2]\n",
    "\n",
    "        return x1.clone(), x2.clone(), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ========= MODEL (same as training) =========\n",
    "class SiameseBranch(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=128, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.branch = SiameseBranch(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.branch(x1)\n",
    "        z2 = self.branch(x2)\n",
    "        return z1, z2\n",
    "\n",
    "# ========= LOAD MODEL =========\n",
    "model = SiameseNetwork(input_dim=128, embedding_dim=128).to(device)\n",
    "state = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "print(f\"Loaded model from {MODEL_PATH}\")\n",
    "\n",
    "# ========= FUNCTION TO COLLECT DISTANCES AND LABELS =========\n",
    "def collect_distances_and_labels(pairs_csv):\n",
    "    dataset = CachedFingerprintPairsDataset(pairs_csv)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    all_dist = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in loader:\n",
    "            x1 = x1.to(device, non_blocking=True)\n",
    "            x2 = x2.to(device, non_blocking=True)\n",
    "            y  = y.to(device, non_blocking=True)\n",
    "\n",
    "            z1, z2 = model(x1, x2)\n",
    "            d = F.pairwise_distance(z1, z2)\n",
    "\n",
    "            all_dist.append(d.cpu().numpy())\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "\n",
    "    all_dist = np.concatenate(all_dist, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    print(f\"Collected {len(all_dist)} distances from {os.path.basename(pairs_csv)}\")\n",
    "    return all_dist, all_labels\n",
    "\n",
    "# ========= EVAL ON VAL: FIND BEST THRESHOLD =========\n",
    "print(\"\\n--- Collecting distances on VAL set ---\")\n",
    "val_dist, val_labels = collect_distances_and_labels(VAL_CSV)\n",
    "\n",
    "# Sweep thresholds between min and max distance\n",
    "t_min, t_max = float(val_dist.min()), float(val_dist.max())\n",
    "print(f\"Val distance range: [{t_min:.4f}, {t_max:.4f}]\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_thr = None\n",
    "\n",
    "thresholds = np.linspace(t_min, t_max, num=200)  # 200 points in range\n",
    "\n",
    "for thr in thresholds:\n",
    "    # prediction: match if distance <= thr\n",
    "    preds = (val_dist <= thr).astype(np.float32)\n",
    "\n",
    "    correct = (preds == val_labels).sum()\n",
    "    acc = correct / len(val_labels)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thr = thr\n",
    "\n",
    "print(f\"Best threshold on VAL: {best_thr:.4f}, accuracy={best_acc:.4f}\")\n",
    "\n",
    "# Optional: compute TPR/FPR at this threshold\n",
    "pos_mask = (val_labels == 1)\n",
    "neg_mask = (val_labels == 0)\n",
    "\n",
    "tpr = ((val_dist[pos_mask] <= best_thr).sum() / pos_mask.sum())  # genuine accept rate\n",
    "fpr = ((val_dist[neg_mask] <= best_thr).sum() / neg_mask.sum())  # false accept rate\n",
    "\n",
    "print(f\"VAL TPR (GAR) at best_thr: {tpr:.4f}\")\n",
    "print(f\"VAL FPR (FAR) at best_thr: {fpr:.4f}\")\n",
    "\n",
    "# ========= EVAL ON TEST WITH BEST THRESHOLD =========\n",
    "print(\"\\n--- Collecting distances on TEST set ---\")\n",
    "test_dist, test_labels = collect_distances_and_labels(TEST_CSV)\n",
    "\n",
    "preds_test = (test_dist <= best_thr).astype(np.float32)\n",
    "correct_test = (preds_test == test_labels).sum()\n",
    "acc_test = correct_test / len(test_labels)\n",
    "\n",
    "pos_mask_t = (test_labels == 1)\n",
    "neg_mask_t = (test_labels == 0)\n",
    "\n",
    "tpr_t = ((test_dist[pos_mask_t] <= best_thr).sum() / pos_mask_t.sum())\n",
    "fpr_t = ((test_dist[neg_mask_t] <= best_thr).sum() / neg_mask_t.sum())\n",
    "\n",
    "print(f\"\\nTEST results at best_thr={best_thr:.4f}:\")\n",
    "print(f\"  Accuracy: {acc_test:.4f}\")\n",
    "print(f\"  TPR (GAR): {tpr_t:.4f}\")\n",
    "print(f\"  FPR (FAR): {fpr_t:.4f}\")\n",
    "\n",
    "# ========= OPTIONAL: SAVE DISTANCES FOR PLOTTING LATER =========\n",
    "OUT_METRICS = r\"D:\\5th sem\\mini_project\\models\\snn_eval_distances.npz\"\n",
    "np.savez(\n",
    "    OUT_METRICS,\n",
    "    val_dist=val_dist,\n",
    "    val_labels=val_labels,\n",
    "    test_dist=test_dist,\n",
    "    test_labels=test_labels,\n",
    "    best_thr=best_thr\n",
    ")\n",
    "print(f\"\\nSaved distances and labels to {OUT_METRICS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2d12d-3652-4065-857c-88def79c7ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf-env)",
   "language": "python",
   "name": "hf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
